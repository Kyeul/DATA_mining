{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "총 개수:  807\n"
     ]
    }
   ],
   "source": [
    "#크롤링을 통해 책의 고유 ID를 수집\n",
    "num = 1\n",
    "itemids = []\n",
    "i = 1\n",
    "\n",
    "#책 id를 추출하는 정규식\n",
    "compiler1 = re.compile('/Product/Goods/\\d{5,8}')\n",
    "compiler2 = re.compile('\\d{5,8}')\n",
    "\n",
    "while True:\n",
    "    url = 'http://www.yes24.com/searchcorner/Search?keywordAd=&keyword=&query=%c0%ce%b0%f8%c1%f6%b4%c9&domain=BOOK&page_size=120&PageNumber={}&scode=012'\n",
    "    page = requests.get(url .format(num))\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    bookInfo = soup.select('p.goods_name a')\n",
    "    if len(bookInfo) <= 0:\n",
    "        print('Done!')\n",
    "        break\n",
    "    \n",
    "    for info in bookInfo:\n",
    "        try:\n",
    "            itemid=compiler1.search(str(info)).group()\n",
    "            itemids.append(compiler2.search(itemid).group())\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    num += 1\n",
    "print('총 개수: ', len(itemids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 생성\n",
    "columns = ['제목','부제', '저자', '분야', '출판일', '출판사', '가격', '목차', '소개', '출판사제공'] #데이터 프레임의 컬럼명 \n",
    "dataFrame = pd.DataFrame(columns=columns) #dataFrame을 만들고 지정한 column명으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 책의 상세 페이지에 접근하여 데이터 크롤링\n",
    "n = 0 #dataframe index 초기화\n",
    "year = 2012  #검색 기간\n",
    "for i in itemids:\n",
    "    url = 'http://www.yes24.com/Product/Goods/{}?scode=032&OzSrank=1'\n",
    "    page = requests.get(url .format(i))\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    #출간일\n",
    "    try:\n",
    "        date = soup.find('span', attrs={'class':'gd_date'}).text\n",
    "        date = re.sub('년|월|일|\\ ', '', date) #한글 단위를 모두 제거\n",
    "        if int(date[:4]) < year:\n",
    "            continue\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    title = soup.find('h2', attrs={'class':'gd_name'}).text #제목을 가져온다.\n",
    "    if '세트' in title:\n",
    "        continue\n",
    "    try:\n",
    "        sub_title = soup.find('h3', attrs={'class':'gd_nameE'}).text  #부제목을 가져온다.\n",
    "    except AttributeError:\n",
    "        sub_title = None\n",
    "        \n",
    "    try:\n",
    "        author = soup.select('span.gd_auth a')[0].text #저자 정보\n",
    "    except IndexError:\n",
    "        try:\n",
    "            author = soup.select('span.gd_auth')[0].text #저자 정보\n",
    "            author = re.sub('\\\\r|\\\\n|\\ ', '', author)\n",
    "        except IndexError:\n",
    "            author = None\n",
    "            \n",
    "    feild = soup.select('div.gd_infoSet a')[1].text  #분야를 가져온다.\n",
    "\n",
    "    \n",
    "\n",
    "    publisher = soup.select('span.gd_pub')[0].text #출판사\n",
    "\n",
    "    #가격\n",
    "    price = soup.find('em', attrs={'class':'yes_m'}).text\n",
    "    price = re.sub(',|원', '', price)\n",
    "    \n",
    "    #목차\n",
    "    try:\n",
    "        contents = soup.select('textarea.txtContentText')[1].text\n",
    "    except IndexError:\n",
    "        try:\n",
    "            contents = soup.select('textarea.txtContentText')[0].text\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    introduction = soup.select('textarea.txtContentText')[0].text #책소개\n",
    "    \n",
    "    #출판사 소개\n",
    "    try:\n",
    "        introduction_2 = soup.select('div#infoset_pubReivew')[0].text \n",
    "        introduction_2 = re.sub('출판사 리뷰|\\\\n|펼쳐보기접어보기', ' ', introduction_2) #불용어 제거\n",
    "    except IndexError:\n",
    "        introduction_2 = None\n",
    "    \n",
    "    #['제목','부제', '저자', '분야', '출판일', '출판사', '가격', '목차', '소개', '출판사제공']\n",
    "    dataFrame.loc[n] = [title, sub_title, author, feild, date, publisher, price, contents, introduction, introduction_2]\n",
    "    n+= 1;\n",
    "\n",
    "print('데이터 개수: ', len(dataFrame))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.to_csv(r'AI_book_data.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
